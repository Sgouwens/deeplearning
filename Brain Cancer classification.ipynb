{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will train a convolutional architecture in Pytorch to label brain MRI images with a type of malignancy of types:\n",
    "\n",
    "glioma - meningioma - notumor - pituitary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from typing import List, Tuple\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all images have the same size. We resize to 512 x 512 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, transform: transforms.Compose = None, target_size: Tuple[int, int] = (512, 512)):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size  \n",
    "        \n",
    "        self._X: List[torch.Tensor] = []\n",
    "        self._y: List[int] = []\n",
    "        self.image_paths: List[str] = [] \n",
    "        self.label_map = {\n",
    "            'glioma': 0,\n",
    "            'meningioma': 1,\n",
    "            'notumor': 2,\n",
    "            'pituitary': 3\n",
    "        }\n",
    "        self.label_map_inv = {v: k for k, v in self.label_map.items()}\n",
    "        \n",
    "        self.basic_transform = transforms.Compose([\n",
    "            transforms.Resize(self.target_size, interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        # Load data into memory\n",
    "        self._load_data()\n",
    "    \n",
    "    def _load_data(self):\n",
    "        \"\"\"Loads and preprocesses the dataset.\"\"\"\n",
    "        for class_name, label in self.label_map.items():\n",
    "            class_folder = os.path.join(self.root_dir, class_name)\n",
    "            if os.path.isdir(class_folder):\n",
    "                for filename in os.listdir(class_folder):\n",
    "                    image_path = os.path.join(class_folder, filename)\n",
    "                    if os.path.isfile(image_path):\n",
    "                        try:\n",
    "                            image = Image.open(image_path).convert('RGB')\n",
    "                            \n",
    "                            image_tensor = self.basic_transform(image)\n",
    "                            \n",
    "                            # Verify tensor size\n",
    "                            expected_shape = (3, *self.target_size)\n",
    "                            if image_tensor.shape != expected_shape:\n",
    "                                print(f\"Error: Tensor shape is {image_tensor.shape}, expected {expected_shape}\")\n",
    "                                continue\n",
    "                            \n",
    "                            self._X.append(image_tensor)\n",
    "                            self._y.append(label)\n",
    "                            self.image_paths.append(image_path)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of images in the dataset.\"\"\"\n",
    "        return len(self._X)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Returns a single image tensor and its corresponding label.\"\"\"\n",
    "        image_tensor = self._X[idx]\n",
    "        label = self._y[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "        \n",
    "        return image_tensor, label\n",
    "\n",
    "    def plot_random_images(self, num_images=4, figsize=(12, 3)):\n",
    "        \"\"\"Plot a random sample of images with their labels.\"\"\"\n",
    "        fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
    "        random_indices = random.sample(range(len(self)), num_images)\n",
    "        \n",
    "        for i, idx in enumerate(random_indices):\n",
    "            image_tensor = self._X[idx] \n",
    "            label = self._y[idx]\n",
    "            \n",
    "            image_array = image_tensor.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "            \n",
    "            ax = axes[i]\n",
    "            ax.imshow(image_array)\n",
    "            ax.set_title(f\"Label: {self.label_map_inv[label]}\")\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train = BrainTumorDataset(root_dir='data/brain-tumor-mri-dataset/training', transform=train_transform)\n",
    "test = BrainTumorDataset(root_dir='data/brain-tumor-mri-dataset/testing', transform=train_transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test, batch_size=16, shuffle=True)\n",
    "\n",
    "train.plot_random_images(num_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 3, 2, 1, 3, 0, 1, 1, 1, 1, 0, 2, 1, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2),    # 512 -> 254\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=5, stride=5),   # 254 -> 50\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=5, stride=5),   # 50 -> 10\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=5, stride=2),   # 10 -> 3\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 3 * 3, 4)\n",
    "        )\n",
    "\n",
    "        self.training_accuracy = []\n",
    "        self.testing_accuracy = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BrainTumorClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "num_epochs = 40\n",
    "\n",
    "# model.load_state_dict(torch.load(\"simple brain cancer convolutional model2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_integrated_gradient(batch_x, model, baseline=None, steps=100, device='cpu'):\n",
    "    model.eval()\n",
    "    batch_x = batch_x.to(device)\n",
    "    \n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(batch_x).to(device)\n",
    "    else:\n",
    "        baseline = baseline.to(device)\n",
    "\n",
    "    # Scale inputs and compute gradients\n",
    "    scaled_inputs = [baseline + (float(i) / steps) * (batch_x - baseline) for i in range(1, steps + 1)]\n",
    "\n",
    "    integrated_gradients = torch.zeros_like(batch_x).to(device)\n",
    "    \n",
    "    for scaled_x in tqdm(scaled_inputs, desc=\"Computing IG\"):\n",
    "        scaled_x.requires_grad_(True)\n",
    "        \n",
    "        output = model(scaled_x)\n",
    "        \n",
    "        # Assuming classification model with single output per sample\n",
    "        output = output.sum()  # sum over batch to get scalar for grad\n",
    "        \n",
    "        grad = torch.autograd.grad(outputs=output, inputs=scaled_x)[0]\n",
    "        integrated_gradients += grad / steps\n",
    "\n",
    "    integrated_gradients = (batch_x - baseline) * integrated_gradients\n",
    "\n",
    "    return integrated_gradients, integrated_gradients.mean(dim=0)\n",
    "\n",
    "ig, mean_grad = compute_integrated_gradient(batch_x=batch[0].to(device), model=model)\n",
    "\n",
    "\n",
    "ig_gray = ig.mean(dim=1)\n",
    "print(ig_gray.shape)\n",
    "plt.imshow(ig[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_total = [][]\n",
    "\n",
    "model.eval()\n",
    "    for batch in train_loader:\n",
    "        ig_batch = model(train_loader[0].to(device))\n",
    "\n",
    "        ig_total.append(ig_batch.cpu().detach().numpy())\n",
    "\n",
    "        ig_total = np.concatenate(ig_total, axis=0)\n",
    "\n",
    "plt.imshow(ig_total[0][2], cmap=\"gray\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionality below can be made part of the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Loss: 0.1124, Accuracy: 0.9615, Test accuracy: 0.9146, Computation time (min): 3.5\n",
      "Epoch [2/40], Loss: 0.1015, Accuracy: 0.9638, Test accuracy: 0.9108, Computation time (min): 3.8\n",
      "Epoch [3/40], Loss: 0.0984, Accuracy: 0.9667, Test accuracy: 0.8917, Computation time (min): 3.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# For testing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gouwenss\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\gouwenss\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\gouwenss\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\gouwenss\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 62\u001b[0m, in \u001b[0;36mBrainTumorDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     59\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y[idx]\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 62\u001b[0m     image_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_tensor, label\n",
      "File \u001b[1;32mc:\\Users\\gouwenss\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\gouwenss\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gouwenss\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gouwenss\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:1372\u001b[0m, in \u001b[0;36mRandomRotation.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1369\u001b[0m         fill \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fill]\n\u001b[0;32m   1370\u001b[0m angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegrees)\n\u001b[1;32m-> 1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gouwenss\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1132\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;66;03m# due to current incoherence of rotation angle direction between affine and rotate implementations\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;66;03m# we need to set -angle.\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m matrix \u001b[38;5;241m=\u001b[39m _get_inverse_affine_matrix(center_f, \u001b[38;5;241m-\u001b[39mangle, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m], \u001b[38;5;241m1.0\u001b[39m, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m])\n\u001b[1;32m-> 1132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gouwenss\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:669\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, matrix, interpolation, expand, fill)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# grid will be generated on the same device as theta and img\u001b[39;00m\n\u001b[0;32m    667\u001b[0m grid \u001b[38;5;241m=\u001b[39m _gen_affine_grid(theta, w\u001b[38;5;241m=\u001b[39mw, h\u001b[38;5;241m=\u001b[39mh, ow\u001b[38;5;241m=\u001b[39mow, oh\u001b[38;5;241m=\u001b[39moh)\n\u001b[1;32m--> 669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_grid_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gouwenss\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:558\u001b[0m, in \u001b[0;36m_apply_grid_transform\u001b[1;34m(img, grid, mode, fill)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fill \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 558\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m img \u001b[38;5;241m=\u001b[39m grid_sample(img, grid, mode\u001b[38;5;241m=\u001b[39mmode, padding_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Fill with required color\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    starting_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    correct = 0 # For testing\n",
    "    \n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # Adding the test accuracy (at the cost of computational speed)\n",
    "    with torch.no_grad(): # prevents computing gradients and therefore interfering with the backpropagation\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            # _, predicted = torch.max(outputs, 1) == labels\n",
    "            correct += (torch.argmax(outputs, 1) == labels).sum().item()\n",
    "        test_accuracy = correct / len(test._y)\n",
    "\n",
    "    model.training_accuracy.append(epoch_accuracy)\n",
    "    model.testing_accuracy.append(test_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Test accuracy: {test_accuracy:.4f}, Computation time (min): {(time.time() - starting_time)/60:.1f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"simple brain cancer convolutional model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gouwenss\\AppData\\Local\\Temp\\ipykernel_13852\\2467694855.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"simple brain cancer convolutional model2.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(\"simple brain cancer convolutional model2.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain surprisingly good results given that the neural network is very easy. Some questions remain:\n",
    "1. What do the kernels look like? What would they look like is we, for example start off with more channels and a larger kernel size?\n",
    "2. Can we spot which tumors are predicted well or bad? \n",
    "\n",
    "Below we see the confusion matrix of the prediction of the simple model. We observe [....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor() missing 1 required positional arguments: \"data\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multilabel_confusion_matrix, confusion_matrix\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m----> 4\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;66;03m# prevents computing gradients and therefore interfering with the backpropagation\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: tensor() missing 1 required positional arguments: \"data\""
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad(): # prevents computing gradients and therefore interfering with the backpropagation\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        true_labels.append(labels)\n",
    "        predicted_labels.append(torch.argmax(outputs, 1))\n",
    "\n",
    "    predicted_labels = predicted_labels.numpy()\n",
    "    true_labels = true_labels.numpy()\n",
    "    multilabel_confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(true_labels, predicted_labels) # Should add target_names, we need to figure out how to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Old text]\n",
    "The problem of detecting (the type of) tumor may be rather complex for a computer. In a regular MRI photo, many features are shown, such as the folds in the brain, eyes, facial features, etc. It has to distinguish these from a tumor. Due to this complexity, a deeper neural network is expected. As seen as above, even a very simple network yield great result. We will explore what results van be reached with more complex models. Deeper networks pair with vanishing and exploding gradients, therefore, we will use batch normalisation and residual layers.\n",
    "\n",
    "We will do two different options:\n",
    "1. As above, define a model with more layers and complexities.\n",
    "2. Use a pretrained model (Inception-V3 for example), and add some layers to it and train those (training the original network is next to impossible on a CPU)\n",
    "\n",
    "For the second option one needs to notice that many pretrained networks are designed for lower pixel dimensions, mostly 224 x 224. Hence we need to resize the data to that dimension.\n",
    "\n",
    "We start off by option 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is off in this network. It does not seem to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make our own network. We start off by building a c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.convblock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=kernel_size//2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=kernel_size//2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "    \n",
    "class BrainTumorClassifier2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            ConvBlock(3, 16, 5),   # Input: 224, output = floor(((224 + 2 - 1*(5-1) - 1) / 2 + 1) = 111 -> ... 55\n",
    "            ConvBlock(16, 16, 4),  # Input: 111, output = floor(((55 + 2 - 1*(5-1) - 1) / 2 + 1) = 27 -> 13\n",
    "            ConvBlock(16, 16, 3),  # Input: 111, output = floor(((55 + 2 - 1*(5-1) - 1) / 2 + 1) = 7 -> 4 \n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "        in_features = self.model.fc.in_features\n",
    "\n",
    "        # Freezing all weights of resnet18\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Linear(in_features, 128),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(128, 32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(32, 4)\n",
    "        # )\n",
    "\n",
    "        self.classifier = nn.Sequential( # Test with a very simple added sequence of layers, see how fast this is.\n",
    "            nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        logits = self.classifier(y)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the convolutional kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1311"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
